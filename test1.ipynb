{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pyspark\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('practice').getOrCreate()\n",
    "# /usr/lib/jvm/java-11-openjdk-amd64/bin/java\n",
    "# Install java on Ubuntu: https://www.theserverside.com/blog/Coffee-Talk-Java-News-Stories-and-Opinions/How-do-I-install-Java-on-Ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-----------+-----------+\n",
      "|            _c0|        _c1|        _c2|        _c3|\n",
      "+---------------+-----------+-----------+-----------+\n",
      "|          title|  Precision|     Recall|   F1_Score|\n",
      "|          covid|0.508196721|      0.775|0.613861386|\n",
      "|       economic|0.755494505|0.712435233|0.733333333|\n",
      "|      education|0.765957447|0.885245902|0.821292776|\n",
      "|  environmental|0.818897638| 0.79389313| 0.80620155|\n",
      "| foreign policy|0.470588235|        0.8|0.592592593|\n",
      "|     governance|        0.5|0.753926702| 0.60125261|\n",
      "|         health|0.896774194| 0.64953271|0.753387534|\n",
      "|    immigration|0.903614458|0.833333333|0.867052023|\n",
      "|       military| 0.58490566|       0.62|0.601941748|\n",
      "|         safety|       0.75|       0.75|       0.75|\n",
      "|social cultural|0.725099602|0.719367589|0.722222222|\n",
      "|social programs| 0.62704918|0.845303867|       0.72|\n",
      "+---------------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.option('header','true').csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-----------+-----------+\n",
      "|          title|  Precision|     Recall|   F1_Score|\n",
      "+---------------+-----------+-----------+-----------+\n",
      "|          covid|0.508196721|      0.775|0.613861386|\n",
      "|       economic|0.755494505|0.712435233|0.733333333|\n",
      "|      education|0.765957447|0.885245902|0.821292776|\n",
      "|  environmental|0.818897638| 0.79389313| 0.80620155|\n",
      "| foreign policy|0.470588235|        0.8|0.592592593|\n",
      "|     governance|        0.5|0.753926702| 0.60125261|\n",
      "|         health|0.896774194| 0.64953271|0.753387534|\n",
      "|    immigration|0.903614458|0.833333333|0.867052023|\n",
      "|       military| 0.58490566|       0.62|0.601941748|\n",
      "|         safety|       0.75|       0.75|       0.75|\n",
      "|social cultural|0.725099602|0.719367589|0.722222222|\n",
      "|social programs| 0.62704918|0.845303867|       0.72|\n",
      "+---------------+-----------+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- title: string (nullable = true)\n",
      " |-- Precision: string (nullable = true)\n",
      " |-- Recall: string (nullable = true)\n",
      " |-- F1_Score: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark2=SparkSession.builder.appName('practice2').getOrCreate()\n",
    "df= spark2.sparkContext.parallelize([(1,2,3,'abc'),(4,5,6,'def'),\n",
    "(7,8,9,'ghi')]).toDF(['col1','col2','col3','col4'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myData=spark.sparkContext.parallelize([(1,2),(3,4),(5,6),(7,8),(9,10)])\n",
    "myData.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employee=spark.createDataFrame([('1', 'Joe', '70000', '1'),\n",
    "('2', 'Henry', '80000', '2'),\n",
    "('3', 'Sam', '60000', '2'),\n",
    "('4', 'Max', '90000', '1')],\n",
    "['Id', 'Name', 'Sallary','DepartmentId']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from database ***\n",
    "## user infomation\n",
    "user = 'user_name'\n",
    "pw = 'password'\n",
    "## database infomation \n",
    "table_name='table_name'\n",
    "url='jdbc:postgresql://##.###.###.##:5432/dataset?user='+user+'&password='+pw\n",
    "properties = {'driver': 'org.postgresql.Driver', 'password': pw,'user': user}\n",
    "df=spark.read.jdbc(url=url, table=table_name,properties=properties)\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read from database ** connect to redshift\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = # existing SparkContext\n",
    "sql_context = SQLContext(sc)\n",
    "\n",
    "# Read data from a table\n",
    "df = sql_context.read \\\n",
    "    .format(\"io.github.spark_redshift_community.spark.redshift\") \\\n",
    "    .option(\"url\", \"jdbc:redshift://redshifthost:5439/database?user=username&password=pass\") \\\n",
    "    .option(\"dbtable\", \"my_table\") \\\n",
    "    .option(\"tempdir\", \"s3n://path/for/temp/data\") \\\n",
    "    .load()\n",
    "\n",
    "# Read data from a query\n",
    "df = sql_context.read \\\n",
    "    .format(\"io.github.spark_redshift_community.spark.redshift\") \\\n",
    "    .option(\"url\", \"jdbc:redshift://redshifthost:5439/database?user=username&password=pass\") \\\n",
    "    .option(\"query\", \"select x, count(*) my_table group by x\") \\\n",
    "    .option(\"tempdir\", \"s3n://path/for/temp/data\") \\\n",
    "    .load()\n",
    "\n",
    "# Write back to a table\n",
    "df.write \\\n",
    "  .format(\"io.github.spark_redshift_community.spark.redshift\") \\\n",
    "  .option(\"url\", \"jdbc:redshift://redshifthost:5439/database?user=username&password=pass\") \\\n",
    "  .option(\"dbtable\", \"my_table_copy\") \\\n",
    "  .option(\"tempdir\", \"s3n://path/for/temp/data\") \\\n",
    "  .mode(\"error\") \\\n",
    "  .save()\n",
    "\n",
    "# Using IAM Role based authentication\n",
    "df.write \\\n",
    "  .format(\"io.github.spark_redshift_community.spark.redshift\") \\\n",
    "  .option(\"url\", \"jdbc:redshift://redshifthost:5439/database?user=username&password=pass\") \\\n",
    "  .option(\"dbtable\", \"my_table_copy\") \\\n",
    "  .option(\"tempdir\", \"s3n://path/for/temp/data\") \\\n",
    "  .option(\"aws_iam_role\", \"arn:aws:iam::123456789000:role/redshift_iam_role\") \\\n",
    "  .mode(\"error\") \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SQL\n",
    "#Reading data using SQL:\n",
    "## operate it on Dbeaver? \n",
    "CREATE TABLE my_table\n",
    "USING io.github.spark_redshift_community.spark.redshift\n",
    "OPTIONS (\n",
    "  dbtable 'my_table',\n",
    "  tempdir 's3n://path/for/temp/data',\n",
    "  url 'jdbc:redshift://redshifthost:5439/database?user=username&password=pass'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create from list\n",
    "\n",
    "my_list = [['a', 1, 2], ['b', 2, 3],['c', 3, 4]]\n",
    "col_name = ['A', 'B', 'C']\n",
    "spark.createDataFrame(my_list,col_name).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dictonary \n",
    "d = {'A': [0, 1, 0],\n",
    "'B': [1, 0, 1],\n",
    "'C': [1, 0, 0]}\n",
    "\n",
    "spark.createDataFrame(np.array(list(d.values())).T.tolist(),list(d.keys())).show()\n",
    "print(np.array(list(d.values())).T.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+----+\n",
      "|     A|  B|   C|\n",
      "+------+---+----+\n",
      "|  male|  1|null|\n",
      "|female|  2|   3|\n",
      "|  male|  3|   4|\n",
      "+------+---+----+\n",
      "\n",
      "+------+---+---+\n",
      "|     A|  B|  C|\n",
      "+------+---+---+\n",
      "|  male|  1|-99|\n",
      "|female|  2|  3|\n",
      "|  male|  3|  4|\n",
      "+------+---+---+\n",
      "\n",
      "+---+---+----+\n",
      "|  A|  B|   C|\n",
      "+---+---+----+\n",
      "|  1|  1|null|\n",
      "|  0|  2|   3|\n",
      "|  1|  3|   4|\n",
      "+---+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_list = [['male', 1, None], ['female', 2, 3],['male', 3, 4]]\n",
    "ds=spark.createDataFrame(my_list,['A','B','C'])\n",
    "ds.show()\n",
    "ds.fillna(-99).show()\n",
    "ds.na.replace(['male','female'],['1','0']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/29 15:35:28 WARN SparkContext: The path http://api.luftdaten.info/static/v1/data.json has been added already. Overwriting of added paths is not supported in the current version.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 55:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------+--------------------+--------------------+-------------------+\n",
      "|         id|            location|sampling_rate|              sensor|    sensordatavalues|          timestamp|\n",
      "+-----------+--------------------+-------------+--------------------+--------------------+-------------------+\n",
      "|13956925375|{437.0, CH, 0, 65...|         null|{76838, 7, {9, va...|[{31265599129, -1...|2023-01-29 20:22:02|\n",
      "|13956925374|{17.0, NL, 1, 506...|         null|{64373, 1, {14, N...|[{31265599127, 56...|2023-01-29 20:22:02|\n",
      "|13956925373|{53.7, DE, 1, 196...|         null|{33301, 1, {14, N...|[{31265599122, 33...|2023-01-29 20:22:02|\n",
      "|13956925372|{7.7, NL, 1, 3827...|         null|{52337, 7, {9, va...|[{31265599118, 4....|2023-01-29 20:22:02|\n",
      "+-----------+--------------------+-------------+--------------------+--------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "url='http://api.luftdaten.info/static/v1/data.json'\n",
    "from pyspark import SparkFiles\n",
    "spark.sparkContext.addFile(url)\n",
    "ds_1=spark.read.json(\"file://\"+SparkFiles.get('data.json'))\n",
    "ds_1.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- location: struct (nullable = true)\n",
      " |    |-- altitude: string (nullable = true)\n",
      " |    |-- country: string (nullable = true)\n",
      " |    |-- exact_location: long (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- indoor: long (nullable = true)\n",
      " |    |-- latitude: string (nullable = true)\n",
      " |    |-- longitude: string (nullable = true)\n",
      " |-- sampling_rate: long (nullable = true)\n",
      " |-- sensor: struct (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- pin: string (nullable = true)\n",
      " |    |-- sensor_type: struct (nullable = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- manufacturer: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |-- sensordatavalues: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- id: long (nullable = true)\n",
      " |    |    |-- value: string (nullable = true)\n",
      " |    |    |-- value_type: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ds_1.fillna({'sampling_rate':'unknown'}).show(10) # failed to fill with string\n",
    "#ds_1.na.fill({'sampling_rate':'unknown'}).show(10)\n",
    "ds_1.printSchema() # check data type, cannot fill string to sampling_rate because it is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+----+\n",
      "|Newspaper|Sales|   C|\n",
      "+---------+-----+----+\n",
      "|     male|    1|null|\n",
      "|   female|    2|   3|\n",
      "|     male|    3|   4|\n",
      "+---------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# rename columns \n",
    "## rename all columns\n",
    "#ds.toDF('a','b','c').show()\n",
    "## rename part of the columns\n",
    "mapping = {'A':'Newspaper','B':'Sales'}\n",
    "# print(mapping.get('A','A'))\n",
    "new_names=[mapping.get(col,col) for col in ds.columns]\n",
    "ds.toDF(*new_names).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|   C|\n",
      "+----+\n",
      "|null|\n",
      "|   3|\n",
      "|   4|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop columns\n",
    "drop_col=['A','B']\n",
    "ds.drop(*drop_col).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter & and | or != not equale to  == equale to\n",
    "#ds[(ds.colum_name conditon1) & (ds.column_name condition2)] # ds[ds.Newspaper<20].show(4)\n",
    "#ds[(ds.colum_name conditon1) | (ds.column_name condition2)] # or\n",
    "\n",
    "# user filter function\n",
    "# df.filter(\"age > 3\").collect()\n",
    "# output: [Row(age=5, name='Bob')]\n",
    "# df.where(\"age = 2\").collect()\n",
    "# output: [Row(age=2, name='Alice')]\n",
    "# filteer is in list values\n",
    "# df.filter(df.column_name.isin(list_name)).show()\n",
    "# example:\n",
    "#li=[\"OH\",\"CA\",\"DE\"]\n",
    "#df.filter(df.state.isin(li)).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(sum(TV)=884.3000000000001)]\n",
      "Row(sum(TV)=884.3000000000001)\n",
      "884.3000000000001\n"
     ]
    }
   ],
   "source": [
    "# group by function and result\n",
    "ls=ds.groupBy().agg(F.sum('TV')).collect()\n",
    "print(ls)\n",
    "print(ls[0])\n",
    "print(ls[0][0])\n",
    "# [Row(sum(TV)=884.3000000000001)]\n",
    "# Row(sum(TV)=884.3000000000001)\n",
    "# 884.3000000000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+--------------------+\n",
      "|   TV|Radio|Newspaper|sales|             tv_norm|\n",
      "+-----+-----+---------+-----+--------------------+\n",
      "|230.1| 37.8|     11.5|   12|  0.2602058125070677|\n",
      "| 44.5| 39.3|      1.0|   14| 0.05032228881601266|\n",
      "| 17.2| 45.9|      3.0|  4.8|0.019450412755852083|\n",
      "|151.5| 41.3|      4.5| 17.4| 0.17132194956462737|\n",
      "+-----+-----+---------+-----+--------------------+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-----+-----+---------+-----+----+\n",
      "|   TV|Radio|Newspaper|sales|cond|\n",
      "+-----+-----+---------+-----+----+\n",
      "|230.1| 37.8|     11.5|   12|   1|\n",
      "| 44.5| 39.3|      1.0|   14|   2|\n",
      "| 17.2| 45.9|      3.0|  4.8|   3|\n",
      "|151.5| 41.3|      4.5| 17.4|   2|\n",
      "+-----+-----+---------+-----+----+\n",
      "only showing top 4 rows\n",
      "\n",
      "+-----+-----+---------+-----+------------------+\n",
      "|   TV|Radio|Newspaper|sales|            log_tv|\n",
      "+-----+-----+---------+-----+------------------+\n",
      "|230.1| 37.8|     11.5|   12|  5.43851399704132|\n",
      "| 44.5| 39.3|      1.0|   14|3.7954891891721947|\n",
      "| 17.2| 45.9|      3.0|  4.8|2.8449093838194073|\n",
      "|151.5| 41.3|      4.5| 17.4| 5.020585624949424|\n",
      "+-----+-----+---------+-----+------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# with new column. use groupby and aggregate create new column\n",
    "# with column DataFrame.withColumn(colName, col) colName is str and col is column expression\n",
    "# ds.withColumn('column_name',column expression).collect()\n",
    "# DataFrame.collect() returns as a list of row \n",
    "# df.collect() # output: [Row(age=2, name='Alice'), Row(age=5, name='Bob')]\n",
    "ds=spark.read.option(\"header\",\"true\").csv('advertising.csv',)\n",
    "# or \n",
    "# ds=spark.read.csv('filename.csv',header=True,sep=',',\n",
    "# encoding='UTF-8',\n",
    "# comment=None,\n",
    "# inferSchema=True)\n",
    "# group by  \n",
    "# gds=ds.groupby(ds.name)  group by name\n",
    "# sorted(gds.agg{'*':'count'}.collect()) sort by number of records \n",
    "# sorted(gdf.agg(F.min(df.age)).collect()) sort by age\n",
    "ds.withColumn('tv_norm', ds.TV/ds.groupBy().agg(F.sum(\"TV\")).collect()[0][0]).show(4)\n",
    "\n",
    "# create new column depending on other columns\n",
    "ds.withColumn('cond',F.when((ds.TV>100)&(ds.Radio<40),1).when(ds.sales>10,2).otherwise(3)).show(4)\n",
    "\n",
    "ds.withColumn('log_tv',F.log(ds.TV)).show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+---------+-----+-----+\n",
      "|   TV|Radio|Newspaper|sales|tv+10|\n",
      "+-----+-----+---------+-----+-----+\n",
      "|230.1| 37.8|     11.5|   12|240.1|\n",
      "| 44.5| 39.3|      1.0|   14| 54.5|\n",
      "| 17.2| 45.9|      3.0|  4.8| 27.2|\n",
      "|151.5| 41.3|      4.5| 17.4|161.5|\n",
      "+-----+-----+---------+-----+-----+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.withColumn('tv+10',ds.TV+10).show(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic table operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lbc/.local/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/lbc/.local/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    }
   ],
   "source": [
    "leftp = pd.DataFrame({'A': ['A0', 'A1', 'A2', 'A3'],\n",
    "'B': ['B0', 'B1', 'B2', 'B3'],\n",
    "'C': ['C0', 'C1', 'C2', 'C3'],\n",
    "'D': ['D0', 'D1', 'D2', 'D3']},\n",
    "index=[0, 1, 2, 3])\n",
    "rightp = pd.DataFrame({'A': ['A0', 'A1', 'A6', 'A7'],\n",
    "'F': ['B4', 'B5', 'B6', 'B7'],\n",
    "'G': ['C4', 'C5', 'C6', 'C7'],\n",
    "'H': ['D4', 'D5', 'D6', 'D7']},\n",
    "index=[4, 5, 6, 7])\n",
    "\n",
    "lefts=spark.createDataFrame(leftp)\n",
    "rights=spark.createDataFrame(rightp)\n",
    "\n",
    "# another way to create \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+----+----+----+\n",
      "|  A|  B|  C|  D|   F|   G|   H|\n",
      "+---+---+---+---+----+----+----+\n",
      "| A0| B0| C0| D0|  B4|  C4|  D4|\n",
      "| A1| B1| C1| D1|  B5|  C5|  D5|\n",
      "| A2| B2| C2| D2|null|null|null|\n",
      "| A3| B3| C3| D3|null|null|null|\n",
      "+---+---+---+---+----+----+----+\n",
      "\n",
      "+---+----+----+----+---+---+---+\n",
      "|  A|   B|   C|   D|  F|  G|  H|\n",
      "+---+----+----+----+---+---+---+\n",
      "| A0|  B0|  C0|  D0| B4| C4| D4|\n",
      "| A1|  B1|  C1|  D1| B5| C5| D5|\n",
      "| A6|null|null|null| B6| C6| D6|\n",
      "| A7|null|null|null| B7| C7| D7|\n",
      "+---+----+----+----+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# left join\n",
    "# in pandas leftp.merge(rightp,on='A',how='left')\n",
    "lefts.join(rights,on='A',how='left').orderBy('A',ascending=True).show()\n",
    "\n",
    "#right join\n",
    "# in pandas leftp.merge(rightp,on='A',how='right')\n",
    "lefts.join(rights,on='A',how='right').orderBy('A',ascending=True).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+---+---+---+---+\n",
      "|  A|  B|  C|  D|  F|  G|  H|\n",
      "+---+---+---+---+---+---+---+\n",
      "| A0| B0| C0| D0| B4| C4| D4|\n",
      "| A1| B1| C1| D1| B5| C5| D5|\n",
      "+---+---+---+---+---+---+---+\n",
      "\n",
      "+---+----+----+----+----+----+----+\n",
      "|  A|   B|   C|   D|   F|   G|   H|\n",
      "+---+----+----+----+----+----+----+\n",
      "| A0|  B0|  C0|  D0|  B4|  C4|  D4|\n",
      "| A1|  B1|  C1|  D1|  B5|  C5|  D5|\n",
      "| A2|  B2|  C2|  D2|null|null|null|\n",
      "| A3|  B3|  C3|  D3|null|null|null|\n",
      "| A6|null|null|null|  B6|  C6|  D6|\n",
      "| A7|null|null|null|  B7|  C7|  D7|\n",
      "+---+----+----+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#inner join\n",
    "lefts.join(rights,on='A',how='inner').orderBy('A',ascending=True).show()\n",
    "\n",
    "#outer/full join\n",
    "lefts.join(rights,on='A',how='full').orderBy('A',ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat tables \n",
    "my_list = [('a', 2, 3),\n",
    "('b', 5, 6),\n",
    "('c', 8, 9),\n",
    "('a', 2, 3),\n",
    "('b', 5, 6),\n",
    "('c', 8, 9)]\n",
    "col_name = ['col1', 'col2', 'col3']\n",
    "\n",
    "ds=spark.createDataFrame(my_list,schema=col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+------+\n",
      "|col1|col2|col3|concat|\n",
      "+----+----+----+------+\n",
      "|   a|   2|   3|    a2|\n",
      "|   b|   5|   6|    b5|\n",
      "|   c|   8|   9|    c8|\n",
      "|   a|   2|   3|    a2|\n",
      "|   b|   5|   6|    b5|\n",
      "|   c|   8|   9|    c8|\n",
      "+----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.withColumn('concat',F.concat('col1','col2')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+---------+\n",
      "|col1|min(col2)|avg(col3)|\n",
      "+----+---------+---------+\n",
      "|   a|        2|      3.0|\n",
      "|   c|        8|      9.0|\n",
      "|   b|        5|      6.0|\n",
      "+----+---------+---------+\n",
      "\n",
      "+----+----+----+----+\n",
      "|col1|   2|   5|   8|\n",
      "+----+----+----+----+\n",
      "|   c|null|null|  18|\n",
      "|   b|null|  12|null|\n",
      "|   a|   6|null|null|\n",
      "+----+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.groupBy(['col1']).agg({'col2':'min','col3':'avg'}).show()\n",
    "ds.groupBy(['col1']).pivot('col2').sum('col3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lbc/.local/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:474: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/home/lbc/.local/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|  A|  B|  C|\n",
      "+---+---+---+\n",
      "|  a|  m|  1|\n",
      "|  b|  m|  2|\n",
      "|  c|  n|  3|\n",
      "|  d|  n|  6|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# another way to create schema\n",
    "d = {'A':['a','b','c','d'],'B':['m','m','n','n'],'C':[1,2,3,6]}\n",
    "dp = pd.DataFrame(d)\n",
    "ds = spark.createDataFrame(dp)\n",
    "ds.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "dp['rank'] = dp.groupby('B')['C'].rank('dense',ascending=False)\n",
    "w = Window.partitionBy('B').orderBy(ds.C.desc())\n",
    "ds = ds.withColumn('rank',F.rank().over(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>m</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b</td>\n",
       "      <td>m</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c</td>\n",
       "      <td>n</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d</td>\n",
       "      <td>n</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A  B  C  rank\n",
       "0  a  m  1   2.0\n",
       "1  b  m  2   1.0\n",
       "2  c  n  3   2.0\n",
       "3  d  n  6   1.0"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dp.head()\n",
    "ds.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
